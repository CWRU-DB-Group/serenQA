export const Instructions = () => {
  return (
    <div className="max-w-4xl mx-auto mt-12 p-6 bg-white rounded-lg shadow">
      <h3 className="text-2xl font-bold mb-6">Instructions for Expertise Evaluators</h3>

      <div className="space-y-8">
        <div>
          <h4 className="text-xl font-semibold mb-3">How the Questions are Organized</h4>
          <p className="text-gray-700">
            The questions are divided into <strong>5 batches</strong> based on drug categories.
            You can refer to <a href="https://cwru-db-group.github.io/serenQA/batch.json" target="_blank" className="text-blue-600">batch.json</a> to see the categories for each batch.
            Each {"question's"} category is labeled as <span className="text-gray-600">[Category - Category:Subcategory]</span> following the text question.
          </p>
        </div>

        <div>
          <h4 className="text-xl font-semibold mb-3">How the Default Scores are Generated</h4>
          <p className="text-gray-700">
            To evaluate the serendipity of each answer within a question, we utilize three <strong>Large Language Models (LLMs)</strong>: GPT-4O,
            GPT-4O-mini, and Claude-3.5-Sonnet,
            following a specific <a href="https://cwru-db-group.github.io/serenQA/prompt.txt" target="_blank" className="text-blue-600">prompt</a>.
            Scores are normalized and combined using a weighted average -- <strong>GPT-4O</strong> and <strong>GPT-4O-mini</strong>, from the same company, contribute <strong>30%</strong> each to balance their overlap,
            while <strong>Claude-3.5-Sonnet</strong> contributes <strong>40%</strong> to ensure diversity in evaluation.
            The combined score is then scaled to a range of <strong>0</strong> to <strong>20</strong> for consistency and interpretability.
            {/*The <strong>LLM ranking</strong> is generated by evaluating the serendipity of each answer within a question using GPT-4O, GPT-4O-mini, and*/}
            {/*Claude-3.5-Sonnet, following a specific <a href="https://cwru-db-group.github.io/serenQA/prompt.txt" target="_blank" className="text-blue-600">prompt</a> .*/}
            {/*Rankings are based on the mean score across these models.*/}
            {/*The first in line is the most serendipity answer according to the models.*/}
          </p>
        </div>

        <div>
          <h4 className="text-xl font-semibold mb-3">Your Role</h4>
          <p className="text-gray-700 mb-4">Please complete your evaluation following these steps:</p>

          <div className="space-y-4 ml-4">
            <div>
              <p className="font-semibold">1. Review the Question and Rankings:</p>
              <ul className="list-disc ml-8 space-y-1 text-gray-700">
                <li>Select at least <strong>1 batch</strong> that align with your interest or expertise to evaluate.</li>
                <li>Carefully review the <strong>LLM scores</strong> for each question in your selected batch.</li>
                <li>Assess whether you agree with the provided score.</li>
              </ul>
            </div>

            <div>
              <p className="font-semibold">2. Provide Your Input:</p>
              <ul className="list-disc ml-8 space-y-1 text-gray-700">
                <li>If you agree with the scores, keep them <strong>unchanged</strong>.</li>
                <li>If you disagree with any default score, please <strong>adjust</strong> it by providing a new score that better reflects the serendipity based on your expertise.</li>
              </ul>
            </div>

            {/*<div>*/}
            {/*  <p className="font-semibold">3. Format for Your Ranking:</p>*/}
            {/*  <ul className="list-disc ml-8 space-y-1 text-gray-700">*/}
            {/*    <li>Use the numbers corresponding to the answers in the LLM ranking.</li>*/}
            {/*    <li>Separate the numbers with commas (e.g., <strong>2, 1, 3</strong> for Answer (2) {'>'} Answer (1) {'>'} Answer (3)).</li>*/}
            {/*    <li>If you are not confident in ranking all answers, you may rank only a subset.</li>*/}
            {/*  </ul>*/}
            {/*</div>*/}

            <div>
              <p className="font-semibold">3. Confidence Rating:</p>
              <ul className="list-disc ml-8 space-y-1 text-gray-700">
                <li>For each batch, rate your confidence (1–5) in the responses, where <strong>1</strong> indicates the lowest confidence and <strong>5</strong> the highest.</li>
              </ul>
            </div>
          </div>
        </div>

        <div>
          <h4 className="text-xl font-semibold mb-3">Important Notes</h4>
          <ul className="list-decimal ml-5 space-y-2 text-gray-700">
            <li>To make sure your responses are recorded, please log in before beginning. Your login status is displayed in the side bar.</li>
            <li>If you refresh or reload the page, or return to evaluate more batches, please log in again using the same email.</li>
            <li>Batches submitted successfully will be marked with a {'"✓"'}.</li>
            <li>If you submit a batch multiple times, only the last submission will be recorded.</li>
            <li>You may take <strong>Batch 6</strong> as a practice batch to familiarize yourself with the evaluation process.</li>
          </ul>
        </div>
      </div>
    </div>
  )
}